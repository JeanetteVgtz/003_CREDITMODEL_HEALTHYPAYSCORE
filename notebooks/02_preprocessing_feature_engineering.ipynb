{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b625c0f-fe74-44ac-9660-c18351d81a7b",
   "metadata": {},
   "source": [
    "# Data Preprocessing & Feature Engineering\n",
    "\n",
    "**Objective:** Prepare cleaned data for modeling by handling missing values, creating new features, encoding categorical variables, and splitting data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "779164b9-caad-4fa1-85b6-4831c0e2ddff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: (87889, 91)\n",
      "Rows: 87,889\n",
      "Columns: 91\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "# Load cleaned data from notebook 1\n",
    "df = pd.read_csv('../data/processed/loans_cleaned_final.csv')\n",
    "\n",
    "print(f\"Data loaded: {df.shape}\")\n",
    "print(f\"Rows: {df.shape[0]:,}\")\n",
    "print(f\"Columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0fafdcf6-b9c3-4145-8af1-6aa3e346e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with missing values: 14\n",
      "\n",
      "Missing values summary:\n",
      "                    Column  Missing_Count  Missing_Percentage\n",
      "24  mths_since_last_delinq          42231           48.050382\n",
      "64   mths_since_recent_inq           9268           10.545119\n",
      "8                emp_title           5648            6.426288\n",
      "9               emp_length           5627            6.402394\n",
      "75        num_tl_120dpd_2m           4590            5.222497\n",
      "58      mo_sin_old_il_acct           2560            2.912765\n",
      "80        percent_bc_gt_75            971            1.104803\n",
      "55                 bc_util            944            1.074082\n",
      "54          bc_open_to_buy            896            1.019468\n",
      "63    mths_since_recent_bc            858            0.976231\n",
      "40            last_pymnt_d             71            0.080784\n",
      "28              revol_util             36            0.040961\n",
      "42      last_credit_pull_d              3            0.003413\n",
      "72           num_rev_accts              1            0.001138\n"
     ]
    }
   ],
   "source": [
    "# Identify columns with missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing.index,\n",
    "    'Missing_Count': missing.values,\n",
    "    'Missing_Percentage': missing_pct.values\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "print(f\"Columns with missing values: {len(missing_df)}\")\n",
    "print(\"\\nMissing values summary:\")\n",
    "print(missing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e2bd6c2-10e4-4395-b8d5-981ce4678c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values handled\n",
      "Remaining missing values: 0\n",
      "Shape: (87889, 91)\n"
     ]
    }
   ],
   "source": [
    "# Strategy for each column:\n",
    "\n",
    "# 1. mths_since_last_delinq (48% missing) - Missing means NO delinquency\n",
    "df['mths_since_last_delinq'].fillna(999, inplace=True)\n",
    "df['has_delinq'] = (df['mths_since_last_delinq'] != 999).astype(int)\n",
    "\n",
    "# 2. mths_since_recent_inq (10.5% missing) - Missing means no recent inquiry\n",
    "df['mths_since_recent_inq'].fillna(999, inplace=True)\n",
    "\n",
    "# 3. emp_title (6.4% missing) - Drop this column (too many unique values, not useful)\n",
    "if 'emp_title' in df.columns:\n",
    "    df.drop('emp_title', axis=1, inplace=True)\n",
    "\n",
    "# 4. emp_length (6.4% missing) - Fill with mode (most common value)\n",
    "if df['emp_length'].isnull().any():\n",
    "    df['emp_length'].fillna(df['emp_length'].mode()[0], inplace=True)\n",
    "\n",
    "# 5. num_tl_120dpd_2m (5.2% missing) - Fill with 0 (no delinquencies)\n",
    "df['num_tl_120dpd_2m'].fillna(0, inplace=True)\n",
    "\n",
    "# 6. mo_sin_old_il_acct (2.9% missing) - Fill with median\n",
    "df['mo_sin_old_il_acct'].fillna(df['mo_sin_old_il_acct'].median(), inplace=True)\n",
    "\n",
    "# 7. percent_bc_gt_75 (1.1% missing) - Fill with median\n",
    "df['percent_bc_gt_75'].fillna(df['percent_bc_gt_75'].median(), inplace=True)\n",
    "\n",
    "# 8. bc_util (1.1% missing) - Fill with median\n",
    "df['bc_util'].fillna(df['bc_util'].median(), inplace=True)\n",
    "\n",
    "# 9. bc_open_to_buy (1.0% missing) - Fill with median\n",
    "df['bc_open_to_buy'].fillna(df['bc_open_to_buy'].median(), inplace=True)\n",
    "\n",
    "# 10. mths_since_recent_bc (1.0% missing) - Fill with median\n",
    "df['mths_since_recent_bc'].fillna(df['mths_since_recent_bc'].median(), inplace=True)\n",
    "\n",
    "# 11-14. Low missing (<0.1%) - Fill with median/mode\n",
    "df['last_pymnt_d'].fillna(df['last_pymnt_d'].mode()[0], inplace=True)\n",
    "df['revol_util'].fillna(df['revol_util'].median(), inplace=True)\n",
    "df['last_credit_pull_d'].fillna(df['last_credit_pull_d'].mode()[0], inplace=True)\n",
    "df['num_rev_accts'].fillna(df['num_rev_accts'].median(), inplace=True)\n",
    "\n",
    "print(\"Missing values handled\")\n",
    "print(f\"Remaining missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "431cb618-aedf-46cb-8425-62e9e95b6bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE ENGINEERING\n",
      "============================================================\n",
      "\n",
      "Feature engineering complete\n",
      "Shape: (87889, 93)\n",
      "\n",
      "New features created:\n",
      "  1. loan_to_income\n",
      "  2. installment_to_income\n",
      "  3. credit_history_years\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Loan-to-Income ratio\n",
    "df['loan_to_income'] = df['loan_amnt'] / df['annual_inc']\n",
    "\n",
    "# 2. Installment-to-Income ratio (monthly payment burden)\n",
    "df['installment_to_income'] = df['installment'] / (df['annual_inc'] / 12)\n",
    "\n",
    "# 3. Credit age (convert earliest_cr_line to years)\n",
    "df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%Y', errors='coerce')\n",
    "reference_date = pd.to_datetime('2018-12-31')\n",
    "df['credit_history_years'] = ((reference_date - df['earliest_cr_line']).dt.days / 365.25)\n",
    "df.drop('earliest_cr_line', axis=1, inplace=True)\n",
    "\n",
    "print(f\"\\nFeature engineering complete\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nNew features created:\")\n",
    "print(\"  1. loan_to_income\")\n",
    "print(\"  2. installment_to_income\")\n",
    "print(\"  3. credit_history_years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7de5f10a-f648-4593-a99c-342b4b8ffb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns to encode: 18\n",
      "\n",
      "Categorical columns:\n",
      "  term: 2 unique values\n",
      "  grade: 7 unique values\n",
      "  sub_grade: 35 unique values\n",
      "  emp_length: 11 unique values\n",
      "  home_ownership: 4 unique values\n",
      "  verification_status: 3 unique values\n",
      "  issue_d: 3 unique values\n",
      "  pymnt_plan: 1 unique values\n",
      "  purpose: 12 unique values\n",
      "  zip_code: 878 unique values\n",
      "  addr_state: 49 unique values\n",
      "  initial_list_status: 2 unique values\n",
      "  last_pymnt_d: 42 unique values\n",
      "  last_credit_pull_d: 42 unique values\n",
      "  application_type: 2 unique values\n",
      "  hardship_flag: 1 unique values\n",
      "  disbursement_method: 1 unique values\n",
      "  debt_settlement_flag: 2 unique values\n"
     ]
    }
   ],
   "source": [
    "# Identify categorical columns that need encoding\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove target and ID columns from encoding\n",
    "categorical_cols = [col for col in categorical_cols if col not in ['default']]\n",
    "\n",
    "print(f\"Categorical columns to encode: {len(categorical_cols)}\")\n",
    "print(\"\\nCategorical columns:\")\n",
    "for col in categorical_cols:\n",
    "    n_unique = df[col].nunique()\n",
    "    print(f\"  {col}: {n_unique} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc3613f-01d4-47fd-bb71-67c6a48ba871",
   "metadata": {},
   "source": [
    "---\n",
    "## Categorical Variables - Encoding Strategy\n",
    "\n",
    "### Columns to Drop:\n",
    "**Single-value columns (no information):**\n",
    "- `pymnt_plan`: All same value\n",
    "- `hardship_flag`: All same value  \n",
    "- `disbursement_method`: All same value\n",
    "\n",
    "**Date columns (not predictive for future loans):**\n",
    "- `issue_d`: Loan issue date\n",
    "- `last_pymnt_d`: Last payment date\n",
    "- `last_credit_pull_d`: Last credit pull date\n",
    "\n",
    "**Too granular:**\n",
    "- `zip_code`: 878 unique values, `addr_state` captures geography better\n",
    "\n",
    "### Columns to Encode:\n",
    "**Ordinal (order matters):**\n",
    "- `grade`: A → G (credit quality)\n",
    "- `sub_grade`: A1 → G5 (finer credit quality)\n",
    "- `emp_length`: Years of employment\n",
    "\n",
    "**Nominal (no order):**\n",
    "- `term`: 36 months / 60 months\n",
    "- `home_ownership`: RENT, OWN, MORTGAGE, OTHER\n",
    "- `verification_status`: Verified, Source Verified, Not Verified\n",
    "- `purpose`: Loan purpose (debt consolidation, credit card, etc.)\n",
    "- `addr_state`: US state\n",
    "- `initial_list_status`: w (whole) / f (fractional)\n",
    "- `application_type`: Individual / Joint\n",
    "- `debt_settlement_flag`: Y / N\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c1930ed-4fa6-43b3-b12b-7d2aa9ee0fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 3 single-value columns:\n",
      "['pymnt_plan', 'hardship_flag', 'disbursement_method']\n",
      "\n",
      "Dropping 3 date columns:\n",
      "['issue_d', 'last_pymnt_d', 'last_credit_pull_d']\n",
      "\n",
      "Dropping zip_code (too granular)\n",
      "\n",
      "============================================================\n",
      "Remaining categorical columns to encode: 11\n",
      "  term: 2 unique values\n",
      "  grade: 7 unique values\n",
      "  sub_grade: 35 unique values\n",
      "  emp_length: 11 unique values\n",
      "  home_ownership: 4 unique values\n",
      "  verification_status: 3 unique values\n",
      "  purpose: 12 unique values\n",
      "  addr_state: 49 unique values\n",
      "  initial_list_status: 2 unique values\n",
      "  application_type: 2 unique values\n",
      "  debt_settlement_flag: 2 unique values\n",
      "\n",
      "Current shape: (87889, 86)\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with single value (no variance)\n",
    "single_value_cols = [col for col in categorical_cols if df[col].nunique() == 1]\n",
    "print(f\"Dropping {len(single_value_cols)} single-value columns:\")\n",
    "print(single_value_cols)\n",
    "df.drop(columns=single_value_cols, inplace=True)\n",
    "\n",
    "# Drop date columns (not predictive of future loans)\n",
    "date_cols = ['issue_d', 'last_pymnt_d', 'last_credit_pull_d']\n",
    "print(f\"\\nDropping {len(date_cols)} date columns:\")\n",
    "print(date_cols)\n",
    "df.drop(columns=date_cols, inplace=True)\n",
    "\n",
    "# Drop zip_code (too many unique values, addr_state captures geography)\n",
    "print(f\"\\nDropping zip_code (too granular)\")\n",
    "df.drop(columns=['zip_code'], inplace=True)\n",
    "\n",
    "# Update categorical list\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Remaining categorical columns to encode: {len(categorical_cols)}\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"  {col}: {df[col].nunique()} unique values\")\n",
    "    \n",
    "print(f\"\\nCurrent shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f7e76cf-9125-484f-91e3-80355e5abe66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordinal encoding complete\n",
      "Binary encoding complete\n",
      "One-hot encoding complete\n",
      "State frequency encoding complete\n",
      "\n",
      "Final shape: (87889, 99)\n",
      "All columns are now numeric\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Ordinal encoding for ordered categories\n",
    "ordinal_mappings = {\n",
    "    'grade': {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7},\n",
    "    'emp_length': {'< 1 year': 0, '1 year': 1, '2 years': 2, '3 years': 3, \n",
    "                   '4 years': 4, '5 years': 5, '6 years': 6, '7 years': 7,\n",
    "                   '8 years': 8, '9 years': 9, '10+ years': 10}\n",
    "}\n",
    "\n",
    "for col, mapping in ordinal_mappings.items():\n",
    "    df[col] = df[col].map(mapping)\n",
    "\n",
    "# For sub_grade, extract numeric part (A1=1, A2=2, ... G5=35)\n",
    "grade_map = {'A': 0, 'B': 5, 'C': 10, 'D': 15, 'E': 20, 'F': 25, 'G': 30}\n",
    "df['sub_grade'] = df['sub_grade'].apply(lambda x: grade_map[x[0]] + int(x[1]))\n",
    "\n",
    "print(\"Ordinal encoding complete\")\n",
    "\n",
    "# Label encoding for binary categories\n",
    "binary_cols = ['term', 'initial_list_status', 'application_type', 'debt_settlement_flag']\n",
    "\n",
    "for col in binary_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "\n",
    "print(\"Binary encoding complete\")\n",
    "\n",
    "# One-hot encoding for nominal categories with few values\n",
    "nominal_cols = ['home_ownership', 'verification_status', 'purpose']\n",
    "\n",
    "df = pd.get_dummies(df, columns=nominal_cols, drop_first=True)\n",
    "\n",
    "print(\"One-hot encoding complete\")\n",
    "\n",
    "# For addr_state (49 values), use frequency encoding\n",
    "state_freq = df['addr_state'].value_counts() / len(df)\n",
    "df['addr_state'] = df['addr_state'].map(state_freq)\n",
    "\n",
    "print(\"State frequency encoding complete\")\n",
    "\n",
    "print(f\"\\nFinal shape: {df.shape}\")\n",
    "print(f\"All columns are now numeric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7821638f-f863-43b2-a654-bfda08404ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL DATA QUALITY CHECK\n",
      "============================================================\n",
      "Missing values: 0\n",
      "Infinite values: 0\n",
      "\n",
      "Data types:\n",
      "float64    74\n",
      "bool       16\n",
      "int32       5\n",
      "int64       4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Target variable (default):\n",
      "  Class 0 (non-default): 70,287 (79.97%)\n",
      "  Class 1 (default): 17,602 (20.03%)\n",
      "\n",
      "Final dataset shape: (87889, 99)\n"
     ]
    }
   ],
   "source": [
    "# Final data quality checks\n",
    "print(\"FINAL DATA QUALITY CHECK\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Check for infinite values\n",
    "print(f\"Infinite values: {np.isinf(df.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "# Check data types\n",
    "print(f\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Verify target variable\n",
    "print(f\"\\nTarget variable (default):\")\n",
    "print(f\"  Class 0 (non-default): {(df['default'] == 0).sum():,} ({(df['default'] == 0).mean()*100:.2f}%)\")\n",
    "print(f\"  Class 1 (default): {(df['default'] == 1).sum():,} ({(df['default'] == 1).mean()*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd527f03-a6b9-48d5-8390-66f4a6193547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 12 data leakage features:\n",
      "  - total_pymnt\n",
      "  - total_pymnt_inv\n",
      "  - total_rec_prncp\n",
      "  - total_rec_int\n",
      "  - total_rec_late_fee\n",
      "  - recoveries\n",
      "  - collection_recovery_fee\n",
      "  - last_pymnt_amnt\n",
      "  - out_prncp\n",
      "  - out_prncp_inv\n",
      "  - last_fico_range_high\n",
      "  - last_fico_range_low\n",
      "\n",
      "Shape after removing leakage: (87889, 87)\n"
     ]
    }
   ],
   "source": [
    "# Remove data leakage features (known only AFTER loan outcome)\n",
    "leakage_features = [\n",
    "    'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int', \n",
    "    'total_rec_late_fee', 'recoveries', 'collection_recovery_fee',\n",
    "    'last_pymnt_amnt', 'out_prncp', 'out_prncp_inv', 'last_fico_range_high', 'last_fico_range_low'\n",
    "]\n",
    "\n",
    "# Check which exist and remove them\n",
    "leakage_present = [col for col in leakage_features if col in df.columns]\n",
    "\n",
    "print(f\"Removing {len(leakage_present)} data leakage features:\")\n",
    "for col in leakage_present:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "df.drop(columns=leakage_present, inplace=True)\n",
    "\n",
    "print(f\"\\nShape after removing leakage: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1888257-560e-4c73-ad3d-1120f9eba26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (87889, 86)\n",
      "Target shape: (87889,)\n",
      "\n",
      "============================================================\n",
      "TRAIN/VALIDATION/TEST SPLIT\n",
      "============================================================\n",
      "Training set:   61,522 rows (70.0%)\n",
      "Validation set: 13,183 rows (15.0%)\n",
      "Test set:       13,184 rows (15.0%)\n",
      "\n",
      "============================================================\n",
      "DEFAULT RATE IN EACH SET\n",
      "============================================================\n",
      "Training:   20.03%\n",
      "Validation: 20.03%\n",
      "Test:       20.03%\n"
     ]
    }
   ],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('default', axis=1)\n",
    "y = df['default']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# First split: 70% train, 30% temp (for validation + test)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: Split temp into 50% validation, 50% test (15% each of total)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAIN/VALIDATION/TEST SPLIT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training set:   {X_train.shape[0]:,} rows ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val.shape[0]:,} rows ({X_val.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set:       {X_test.shape[0]:,} rows ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEFAULT RATE IN EACH SET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training:   {y_train.mean()*100:.2f}%\")\n",
    "print(f\"Validation: {y_val.mean()*100:.2f}%\")\n",
    "print(f\"Test:       {y_test.mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cb55721-09da-4eec-949b-2ec812cb463f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature scaling complete\n",
      "Scaled training data shape: (61522, 86)\n",
      "\n",
      "Example - Before scaling (first feature):\n",
      "  Mean: 14367.06\n",
      "  Std: 8597.46\n",
      "\n",
      "After scaling (first feature):\n",
      "  Mean: 0.000000\n",
      "  Std: 1.000008\n"
     ]
    }
   ],
   "source": [
    "# Scale numerical features (important for some models like Neural Networks, Logistic Regression)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data only\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames to keep column names\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"Feature scaling complete\")\n",
    "print(f\"Scaled training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"\\nExample - Before scaling (first feature):\")\n",
    "print(f\"  Mean: {X_train.iloc[:, 0].mean():.2f}\")\n",
    "print(f\"  Std: {X_train.iloc[:, 0].std():.2f}\")\n",
    "print(f\"\\nAfter scaling (first feature):\")\n",
    "print(f\"  Mean: {X_train_scaled.iloc[:, 0].mean():.6f}\")\n",
    "print(f\"  Std: {X_train_scaled.iloc[:, 0].std():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4330c987-7bcf-4695-8ef0-3763daef99c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All processed data saved\n",
      "\n",
      "Saved files:\n",
      "  - X_train.csv, X_val.csv, X_test.csv (unscaled)\n",
      "  - X_train_scaled.csv, X_val_scaled.csv, X_test_scaled.csv (scaled)\n",
      "  - y_train.csv, y_val.csv, y_test.csv (targets)\n",
      "  - scaler.pkl (for future scaling)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save scaled data for modeling\n",
    "X_train_scaled.to_csv('../data/processed/X_train_scaled.csv', index=False)\n",
    "X_val_scaled.to_csv('../data/processed/X_val_scaled.csv', index=False)\n",
    "X_test_scaled.to_csv('../data/processed/X_test_scaled.csv', index=False)\n",
    "\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "y_val.to_csv('../data/processed/y_val.csv', index=False)\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False)\n",
    "\n",
    "# Save unscaled data (for tree-based models that don't need scaling)\n",
    "X_train.to_csv('../data/processed/X_train.csv', index=False)\n",
    "X_val.to_csv('../data/processed/X_val.csv', index=False)\n",
    "X_test.to_csv('../data/processed/X_test.csv', index=False)\n",
    "\n",
    "# Save scaler for future use\n",
    "with open('../data/processed/scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"All processed data saved\")\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  - X_train.csv, X_val.csv, X_test.csv (unscaled)\")\n",
    "print(\"  - X_train_scaled.csv, X_val_scaled.csv, X_test_scaled.csv (scaled)\")\n",
    "print(\"  - y_train.csv, y_val.csv, y_test.csv (targets)\")\n",
    "print(\"  - scaler.pkl (for future scaling)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47df0c5f-1768-4937-9085-0ccc452bb55e",
   "metadata": {},
   "source": [
    "---\n",
    "## Preprocessing Complete\n",
    "\n",
    "### Summary of Actions:\n",
    "\n",
    "**1. Missing Value Handling:**\n",
    "- Filled missing values using median/mode/domain logic\n",
    "- Created flag for delinquency (48% missing = no delinquency)\n",
    "- Dropped `emp_title` (too many unique values)\n",
    "\n",
    "**2. Feature Engineering:**\n",
    "- Created 3 new features:\n",
    "  - `loan_to_income`: Loan burden relative to income\n",
    "  - `installment_to_income`: Monthly payment affordability\n",
    "  - `credit_history_years`: Length of credit history\n",
    "\n",
    "**3. Data Cleaning:**\n",
    "- Dropped single-value columns (no variance)\n",
    "- Dropped date columns (not predictive)\n",
    "- Dropped `zip_code` (too granular)\n",
    "\n",
    "**4. Encoding:**\n",
    "- Ordinal: `grade`, `sub_grade`, `emp_length`\n",
    "- Binary: `term`, `initial_list_status`, `application_type`, `debt_settlement_flag`\n",
    "- One-hot: `home_ownership`, `verification_status`, `purpose`\n",
    "- Frequency: `addr_state`\n",
    "\n",
    "**5. Data Split:**\n",
    "- Training: 61,522 (70%)\n",
    "- Validation: 13,183 (15%)\n",
    "- Test: 13,184 (15%)\n",
    "- Stratified: 20.03% default rate in all sets\n",
    "\n",
    "**6. Feature Scaling:**\n",
    "- StandardScaler (mean=0, std=1)\n",
    "- Saved both scaled and unscaled versions\n",
    "\n",
    "**7. Data Leakage Prevention:**\n",
    "- Removed 10 features known only after loan outcome:\n",
    "  - Payment amounts (total_pymnt, total_rec_prncp, etc.)\n",
    "  - Recovery amounts (recoveries, collection_recovery_fee)\n",
    "  - Outstanding principal (out_prncp, out_prncp_inv)\n",
    "\n",
    "### Ready for Modeling:\n",
    "- 88 features, all numeric (removed 10 data leakage features)\n",
    "- No missing values\n",
    "- No infinite values\n",
    "- Balanced class distribution preserved\n",
    "- **Data leakage removed:** Dropped post-outcome features (total_pymnt, recoveries, etc.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa15135-bade-4fdf-a798-724ae35a9065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
